{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FineTuneWithCustomDataset.ipynb","provenance":[],"authorship_tag":"ABX9TyOFNMlXJGdT52ivk+OL7l8M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XBAoSr1aRq_1"},"source":["# **Import Statements:**"]},{"cell_type":"code","metadata":{"id":"otG2bxKYSAKh","executionInfo":{"status":"ok","timestamp":1603923888531,"user_tz":300,"elapsed":5006,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"0f4ce6fa-9b18-4b63-ef53-9860019b2f5e","colab":{"base_uri":"https://localhost:8080/"}},"source":["%%bash\n","# pip install transformers\n","# pip install datasets\n","# pip install pytorch-lightning\n","# pip install nlp"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting nlp\n","  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.1.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n","Installing collected packages: nlp\n","Successfully installed nlp-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7edpF1_wRjij","executionInfo":{"status":"ok","timestamp":1603923897202,"user_tz":300,"elapsed":343,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"3c2fa52a-fad4-43de-d4b8-325c32dc3725","colab":{"base_uri":"https://localhost:8080/"}},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import pytorch_lightning as pl\n","from torch.utils.data import Dataset, DataLoader\n","# from pytorch_lightning.loggers import WandbLogger\n","from nlp import load_metric\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup,\n","    AutoTokenizer,\n","    AutoModelWithLMHead, \n","    pipeline\n",")\n","\n","from datasets import load_dataset\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/CS196Project\""],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/CS196Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SNpBvEroSeie"},"source":["# **Load Datasets:**"]},{"cell_type":"code","metadata":{"id":"Qh-jjeIcSik0"},"source":["newsroom = load_dataset('newsroom', data_dir='/content/drive/My Drive/CS196Project/newsroom')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1iS179kMTwrx"},"source":["# **Class Definitions**"]},{"cell_type":"code","metadata":{"id":"3mGz9J75T2Ca","executionInfo":{"status":"ok","timestamp":1603923989892,"user_tz":300,"elapsed":855,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}}},"source":["class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparams):\n","        super(T5FineTuner, self).__init__()\n","        self.hparams = hparams        \n","        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","        self.rouge_metric = load_metric('rouge') \n","        \n","        if self.hparams.freeze_embeds:\n","            self.freeze_embeds()\n","        if self.hparams.freeze_encoder:\n","            self.freeze_params(self.model.get_encoder())\n","            assert_all_frozen(self.model.get_encoder())\n","            \n","            \n","        n_observations_per_split = {\n","            \"train\": self.hparams.n_train,\n","            \"validation\": self.hparams.n_val,\n","            \"test\": self.hparams.n_test,\n","        }\n","        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n","        \n","    \n","    def freeze_params(self, model):\n","        for par in model.parameters():\n","            par.requires_grad = False\n","            \n","            \n","    def freeze_embeds(self):\n","        \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n","        try:\n","            self.freeze_params(self.model.model.shared)\n","            for d in [self.model.model.encoder, self.model.model.decoder]:\n","                freeze_params(d.embed_positions)\n","                freeze_params(d.embed_tokens)\n","        except AttributeError:\n","            self.freeze_params(self.model.shared)\n","            for d in [self.model.encoder, self.model.decoder]:\n","                self.freeze_params(d.embed_tokens)\n","    \n","    def lmap(self, f, x):\n","        \"\"\"list(map(f, x))\"\"\"\n","        return list(map(f, x))\n","    \n","\n","    def is_logger(self):\n","        return self.trainer.proc_rank <= 0\n","    \n","    \n","    def parse_score(self, result):\n","        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n","        \n","    def forward(\n","      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n","  ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            lm_labels=lm_labels,\n","    )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            lm_labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","    \n","    \n","    def ids_to_clean_text(self, generated_ids):\n","        gen_text = self.tokenizer.batch_decode(\n","            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        return self.lmap(str.strip, gen_text)\n","    \n","    \n","    def _generative_step(self, batch) :\n","        \n","        t0 = time.time()\n","        \n","        generated_ids = self.model.generate(\n","            batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            use_cache=True,\n","            decoder_attention_mask=batch['target_mask'],\n","            max_length=150, \n","            num_beams=2,\n","            repetition_penalty=2.5, \n","            length_penalty=1.0, \n","            early_stopping=True\n","        )\n","        preds = self.ids_to_clean_text(generated_ids)\n","        target = self.ids_to_clean_text(batch[\"target_ids\"])\n","            \n","        gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n","    \n","        loss = self._step(batch)\n","        base_metrics = {'val_loss': loss}\n","#         rouge: Dict = self.calc_generative_metrics(preds, target)\n","        summ_len = np.mean(self.lmap(len, generated_ids))\n","        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n","        self.rouge_metric.add_batch(preds, target)\n","        \n","#         rouge_results = self.rouge_metric.compute() \n","#         rouge_dict = self.parse_score(rouge_results)\n","#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n","        \n","        return base_metrics\n","    \n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","  \n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self._generative_step(batch)\n","    \n","  \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        \n","        rouge_results = self.rouge_metric.compute() \n","        rouge_dict = self.parse_score(rouge_results)\n","    \n","        tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n","        \n","        ## Clear out the lists for next epoch\n","        self.target_gen= []\n","        self.prediction_gen=[]\n","        return {\"avg_val_loss\": avg_loss, \n","                \"rouge1\" : rouge_results['rouge1'],\n","                \"rougeL\" : rouge_results['rougeL'],\n","                \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","  \n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=False):\n","        if self.trainer.use_tpu:\n","            xm.optimizer_step(optimizer)\n","        else:\n","            optimizer.step()\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","  \n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","    \n","\n","    def train_dataloader(self):   \n","        n_samples = self.n_obs['train']\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", num_samples=n_samples, args=self.hparams)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n","        t_total = (\n","            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","            // self.hparams.gradient_accumulation_steps\n","            * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        n_samples = self.n_obs['validation']\n","        validation_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"validation\", num_samples=n_samples, args=self.hparams)\n","        \n","        return DataLoader(validation_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n","    \n","    \n","    def test_dataloader(self):\n","        n_samples = self.n_obs['test']\n","        test_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"test\", num_samples=n_samples, args=self.hparams)\n","        \n","        return DataLoader(test_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDqiaP0SUe7H"},"source":["class newsroom():\n","    def __init__(self, tokenizer, split, num_samples, input_length, output_length, print_text=False):         \n","        self.dataset = load_dataset('newsroom', split=split, data_dir='/content/drive/My Drive/CS196Project/newsroom')\n","        if num_samples:\n","            self.dataset = self.dataset.select(list(range(0, num_samples)))\n","        self.input_length = input_length\n","        self.tokenizer = tokenizer\n","        self.output_length = output_length\n","        self.print_text = print_text\n","  \n","    def __len__(self):\n","        return self.dataset.shape[0]\n","    \n","    def clean_text(self, text):\n","        text = text.replace('Example of text:', '')\n","        text = text.replace('Example of Summary:', '')\n","        text = text.replace('\\n','')\n","        text = text.replace('``', '')\n","        text = text.replace('\"', '')\n","        \n","        return text\n","    \n","    \n","    def convert_to_features(self, example_batch):\n","        # Tokenize contexts and questions (as pairs of inputs)\n","        \n","        if self.print_text:\n","            print(\"Input Text: \", self.clean_text(example_batch['text']))\n","#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n","#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n","        \n","        input_ = self.clean_text(example_batch['text'])\n","        target_ = self.clean_text(example_batch['headline'])\n","        \n","        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n","                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n","        \n","        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n","                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n","    \n","       \n","        return source, targets\n","  \n","    def __getitem__(self, index):\n","        source, targets = self.convert_to_features(self.dataset[index])\n","        \n","        source_ids = source[\"input_ids\"].squeeze()\n","        target_ids = targets[\"input_ids\"].squeeze()\n","\n","        src_mask    = source[\"attention_mask\"].squeeze()\n","        target_mask = targets[\"attention_mask\"].squeeze()\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n"],"execution_count":null,"outputs":[]}]}