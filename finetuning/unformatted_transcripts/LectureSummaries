Lecture 1: 3Blue1Brown: “Vectors, what even are they?”

	While a Physics student sees a vector as an arrow in space having two qualities (magnitude and direction), a computer science student sees a vector as an ordered list.
Example: Modeling Houses: if we were to model houses using their square footage and price, we would use a 2 dimensional vector since there are 2 types of data (order matters). However, the broader mathematical perspective is that vectors are defined by their operations of vector addition and scalar multiplication. For linear algebra, it helps to view any vector as an arrow sitting on the origin of the coordinate system (not freely like the physics perspective). Let’s redefine the axis and the coordinate system. We start out with 2 axis for our 2D coordinate system, and we make tick marks to indicate the scale of the axis. How far a vector goes in each dimension is given by the number of tick marks it crosses in that direction. For 3D, it is essentially the same process with a Z dimension. Vector addition: if we put two vectors together such that the tail of one is placed on the tip of the other, their sum is given by the vector going from the first vectors tail to the second vectors tip. Vector addition can essentially be broken down into a list of steps: each vector gives the number of steps to take in their respective coordinate directions, and their sum involves taking the sum of those steps. Scalar Multiplication/Scaling: When a vector is multiplied by a number (scalar) stretches or compresses its magnitude by the scalar, and when the scalar is negative, the vector is flipped across the origin as well. The essence of linear algebra is that we can freely move between the geometric interpretation of vectors and the data-based interpretation. It is the language that lets us talk about changes to large amounts of data and space, while having nice geometric analogies.


Lecture 2: Linear Combinations, span, and basis vectors

While we started off by thing of vectors as a number of steps in each direction of the x-y plane, there is another way to interpret a vector’s coordinates: we can view each coordinate as a scalar. Usually, we think of these scalars as scaling the two fundamental vectors of the x-y plan, known as i-hat and j-hat. I-hat points 1 in the x direction, and j-hat points 1 in the y direction. The vector that is described by the pair of coordinates can be thought of as the some of two scaled vectors, and in this case those vectors are i-hat and j-hat. I-hat and j-hat are called the basis vectors of the xy coordinate system, but they are not the only basis vectors we could choose. If we choose a new set of basis, such as one vector pointing up and to the right and one pointing down and to the right, how many vectors can we describe as the sum of scaled versions of those vectors? We can describe every two-dimensional vector. Whenever we make a vector by adding two scaled vectors, we call the result a linear combination of the two vectors. One way to relate this to the concept of lines is that if you fix one of the scalars and allow the other to move, the path drawn out by the tip of the second vector is a line. There are three cases for how the linear combinations of our basis can look. One, which is the most common, is that our vectors draw out the whole xy plane. The second is that our basis vectors point in the same (or completely opposite) directions, in which case they would draw out a line through the two vectors. Finally, if both of our vectors are zero, we would just have a point at the origin. Span: The set of all vectors we can reach with the linear combination of two vectors. Span is connected to the two fundamental vector operations; we can think of the span as the set of vectors that can be reached by performing only these two elementary operations on our two basis vectors. Sometimes we visualize vectors as points, where the point is at the tip of the vector, because thinking about a set of arrows can be unwieldy. Span becomes deeper in more dimensions. In the three dimensional space the span of two vectors tends to trace out a sheet that slices through the origin. However, we can add a third vector in the same way, by adding that vector multiplied by an arbitrary scalar to our linear combination. A couple things can happen with a third vector. If the third vector happens to accidentally fall on the span on the first two, nothing changes. However, if the third vector is not on the span of the first two, the span will trace out all of 3D space. Linearly dependent: when a vector doesn’t add anything to the span, because it already sits on the span of the combination. Linearly independent: when a vector doesn’t sit on the span of the previous vectors, thereby adding something to the span of the vectors. Technical definition of basis of a space: a set of linearly independent vectors that span the full space. Using what we’ve learned, why does this definition make sense?

Lecture 3: Inverse matrices, column space, and null space

Linear algebra is useful because it lets us solve systems of equations. System of equations: we have a list of variables we don’t know, and in each equation, they are scaled by some value. Traditionally when we write a system, we line the variables up, multiplied by their respective scalars, and put the lingering constant on the right. When written like this, we can visualize the linear system as a matrix multiplication equation: we have a matrix containing the coefficients on the left multiplied by a vector containing the variables, and then we have a constant vector on the right. We can think of A as a transformation, and the equations describes searching for a vector, x, which when transformed using A, gives us v. For instance, if you have a system of 2 equations in 2 variables, we’ll have a 2x2 matrix and our variable and solution vectors will each be two-dimensional. There are two possible cases for this multiplication: one where A has a determinant of 0 and squishes space into a lower dimension, or where A has a nonzero determinant. In the later case, there will be exactly one vector x that solves the equation. The way we find this vector is by taking the solution vector and applying the transformation in reverse, denoted by a matrix known as the inverse of A. Example: if A is a counterclockwise rotation of 90 degrees, A inverse is a clockwise rotation of 90 degrees. The core property of this inverse matrix is that if you apply A inverse to A we get the matrix that corresponds to doing nothing, which is called the identity transformation. This algorithm extends to matrices in higher dimensions: as long as the determinant of A is non-zero, there will be a matrix A inverse, and to solve for the variables we just have to multiply the solution vector by the inverse transformation matrix. However, if the determinant is zero, meaning that the transformation given by A squishes space into a lower dimension, there is no function that can un-squish a line. To do this, the function would have to transform each vector into a full line of vectors, which is impossible for a function because they can only map a single input to a single output. However, a solution can still exist if the determinant is 0: if space is squished onto a line with the transformation, for there to be a solution, the solution has to be on that line. Some cases are more restrictive than others; for instance, squishing space onto a line or point is much more restrictive than squishing it onto a plane. We can describe the level of squishing in the zero determinant cases with the term “rank.” Rank: the number of dimensions in the output of a transformation. In the 2x2 case, rank 2 is the best scenario, but for the three dimensional case, rank 2 would mean that we’ve condensed space into a smaller dimension. Column Space: the set of all possible outputs for the Matrix multiplication, also is the span of the columns of the matrix. When the rank is equal to the number of columns, it is called full rank. In a full tank transformation, the only vector that lands on the origin is the zero vector, but that isn’t the case for all transformations. For instance, when the transformation squishes 2D space into a line, there is an entire line of vectors that is compressed to the origin rather than just one. Null Space: the set of vectors that lands on the origin with a transformation, also known as the Kernel. 

