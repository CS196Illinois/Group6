{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine-Tuning GPT2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QWmtbdYHI9HN"},"source":["# Fine-Tuning GPT2 on Colab GPUâ€¦ ForÂ Free!\n","\n","This is a colab notebook for the [associated Medium article](https://medium.com/p/340468c92ed)"]},{"cell_type":"markdown","metadata":{"id":"1ov2wQbRIs8J"},"source":["## Installing Dependencies\n","We would run pip3 install transformers normally in Bash, but because this is in Colab, we have to run it withÂ !"]},{"cell_type":"code","metadata":{"id":"GVQ5Le5kF7CV","executionInfo":{"status":"ok","timestamp":1603681367675,"user_tz":300,"elapsed":22917,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"e585251d-3ab5-4964-d42a-21e361a90114","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# !pip3 install transformers\n","!python -m pip install git+https://github.com/huggingface/transformers.git\n","!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-pbizc8jb\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-pbizc8jb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied (use --upgrade to upgrade): transformers==3.4.0 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.12.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (1.18.5)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.9.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2.23.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.1.94)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2019.12.20)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.4.0) (50.3.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.4.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.4.0) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (1.24.3)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-3.4.0-cp36-none-any.whl size=1275545 sha256=da0734b0e7ba96dc7c51bfa1bbe354f5a1533b80d1dbf015b201752f9c783eea\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-45lma276/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n","Successfully built transformers\n","Collecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/f4/2a3d6aee93ae7fce6c936dda2d7f534ad5f044a21238f85e28f0b205adf0/datasets-1.1.2-py3-none-any.whl (147kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153kB 12.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 26.7MB/s \n","\u001b[?25hCollecting pyarrow>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7MB 204kB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, pyarrow, datasets\n","  Found existing installation: pyarrow 0.14.1\n","    Uninstalling pyarrow-0.14.1:\n","      Successfully uninstalled pyarrow-0.14.1\n","Successfully installed datasets-1.1.2 pyarrow-2.0.0 xxhash-2.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wdfc4qUVKYLj"},"source":["## Getting WikiText Data\n","\n","You can read more about WikiText data here. Overall, there's WikiText-2 and WikiText-103. We're going to use WikiText-2 because it's smaller, and we have limits in terms of how long we can run on GPU, and how much data we can load into memory in Colab. To download and run"]},{"cell_type":"code","metadata":{"id":"kfsUkAZluYMW","executionInfo":{"status":"ok","timestamp":1603922463269,"user_tz":300,"elapsed":16898,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"47d52b29-3da8-4ff6-ac50-3f4e71e7a345","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/CS196Project\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/CS196Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C4RMT_FQIrGi","executionInfo":{"status":"ok","timestamp":1603679219313,"user_tz":300,"elapsed":56799,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"2513673e-386a-4900-83f2-0049ee065d2b","colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["%%bash\n","# wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n","# unzip wikitext-2-raw-v1.zip\n","\n","# wget https://cdn-datasets.huggingface.co/summarization/pegasus_data/newsroom.tar.gz\n","# unzip newsroom.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  wikitext-2-raw-v1.zip\n","   creating: wikitext-2-raw/\n","  inflating: wikitext-2-raw/wiki.test.raw  \n","  inflating: wikitext-2-raw/wiki.valid.raw  \n","  inflating: wikitext-2-raw/wiki.train.raw  \n","Archive:  newsroom.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lEJjg5wkLXgI"},"source":["## Fine-Tuning GPT2\n","\n","HuggingFace actually provides a script to help fine tune models here. We can just download the script by running"]},{"cell_type":"code","metadata":{"id":"E-L4LiHiKdr1","executionInfo":{"status":"ok","timestamp":1603922494918,"user_tz":300,"elapsed":671,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"c3015073-3eae-40c3-ba93-6441348e3223","colab":{"base_uri":"https://localhost:8080/"}},"source":["! wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-10-28 22:01:34--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13076 (13K) [text/plain]\n","Saving to: â€˜run_language_modeling.pyâ€™\n","\n","\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  12.77K  --.-KB/s    in 0.001s  \n","\n","2020-10-28 22:01:34 (9.54 MB/s) - â€˜run_language_modeling.pyâ€™ saved [13076/13076]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sklH4LNoMxRC"},"source":["Now we are ready to fine tune.\n","\n","There are many parameters to the script, and you can understand them by reading the manual. I'm just going to go over the important ones for basic training.\n","\n","- `output_dir` is where the model will be output\n","- `model_type` is what model you want to use. In our case, it's gpt2Â \n","- `model_name_or_path` is the path to the model. If you want to train from scratch, you can leave this blank. In our case, it's also gpt2Â \n","- `do_train` tells it to train\n","- `train_data_file` points to the training file\n","- `do_eval` tells it to evaluate afterwards. Not always required, but good to have\n","- `eval_data_file` points to the evaluation file\n","\n","Some extra ones you MAY care about, but you can also skip this.\n","- `save_steps` is when to save checkpoints. If you have limited memory, you can set this to -1 so it'll skip saving until the end\n","- `per_gpu_train_batch_size` is batch size for GPU. You can increase this if your GPU has enough memory. To be safe, you can start with 1 and ramp it up if you still have memory\n","- `num_train_epochs` is the number of epochs to train. Since we're fine-tuning, I'm going to set this to 2\n"]},{"cell_type":"code","metadata":{"id":"Y6fspISiMx5V","executionInfo":{"status":"ok","timestamp":1603570749391,"user_tz":300,"elapsed":216966,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"a87d3a73-1283-4b84-cfe4-6e2676055f23","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["%%bash\n","export TRAIN_FILE=\"/content/drive/My Drive/CS196Project/newsroom/train.source\"\n","export TEST_FILE=newsroom_test.txt\n","export MODEL_NAME=\"distilbart-cnn-12-6\"\n","export OUTPUT_DIR=output_mine\n","\n","python run_language_modeling.py \\\n","    --output_dir=$OUTPUT_DIR \\\n","    --model_type=$MODEL_NAME \\\n","    --model_name_or_path=$MODEL_NAME \\\n","    --do_train \\\n","    --train_data_file=$TRAIN_FILE \\\n","    --do_eval \\\n","    --eval_data_file=$TEST_FILE \\\n","    --per_gpu_train_batch_size=1 \\\n","    --save_steps=-1 \\\n","    --num_train_epochs=2 \\\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-10-24 20:15:33.623236: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","10/24/2020 20:15:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","10/24/2020 20:15:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output_mine', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct24_20-15-35_6c1e22b6e772', logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output_mine', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:825: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1374: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","10/24/2020 20:15:42 - INFO - filelock -   Lock 140567029244200 acquired on cached_lm_BartTokenizer_1022_newsroom_train.txt.lock\n","10/24/2020 20:15:43 - INFO - filelock -   Lock 140567029244200 released on cached_lm_BartTokenizer_1022_newsroom_train.txt.lock\n","10/24/2020 20:15:43 - INFO - filelock -   Lock 140567010517512 acquired on cached_lm_BartTokenizer_1022_newsroom_test.txt.lock\n","10/24/2020 20:15:44 - INFO - filelock -   Lock 140567010517512 released on cached_lm_BartTokenizer_1022_newsroom_test.txt.lock\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:263: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n","  FutureWarning,\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","\r  0%|          | 0/378 [00:00<?, ?it/s]\r  0%|          | 1/378 [00:00<04:24,  1.42it/s]\r  1%|          | 2/378 [00:01<03:50,  1.63it/s]\r  1%|          | 3/378 [00:01<03:26,  1.82it/s]\r  1%|          | 4/378 [00:01<03:11,  1.95it/s]\r  1%|â–         | 5/378 [00:02<02:59,  2.08it/s]\r  2%|â–         | 6/378 [00:02<02:50,  2.18it/s]\r  2%|â–         | 7/378 [00:03<02:45,  2.24it/s]\r  2%|â–         | 8/378 [00:03<02:40,  2.30it/s]\r  2%|â–         | 9/378 [00:03<02:37,  2.34it/s]\r  3%|â–Ž         | 10/378 [00:04<02:35,  2.37it/s]\r  3%|â–Ž         | 11/378 [00:04<02:34,  2.38it/s]\r  3%|â–Ž         | 12/378 [00:05<02:32,  2.39it/s]\r  3%|â–Ž         | 13/378 [00:05<02:31,  2.41it/s]\r  4%|â–Ž         | 14/378 [00:06<02:31,  2.41it/s]\r  4%|â–         | 15/378 [00:06<02:30,  2.42it/s]\r  4%|â–         | 16/378 [00:06<02:29,  2.42it/s]\r  4%|â–         | 17/378 [00:07<02:29,  2.42it/s]\r  5%|â–         | 18/378 [00:07<02:28,  2.43it/s]\r  5%|â–Œ         | 19/378 [00:08<02:27,  2.43it/s]\r  5%|â–Œ         | 20/378 [00:08<02:28,  2.42it/s]\r  6%|â–Œ         | 21/378 [00:08<02:27,  2.42it/s]\r  6%|â–Œ         | 22/378 [00:09<02:26,  2.43it/s]\r  6%|â–Œ         | 23/378 [00:09<02:26,  2.42it/s]\r  6%|â–‹         | 24/378 [00:10<02:26,  2.41it/s]\r  7%|â–‹         | 25/378 [00:10<02:26,  2.41it/s]\r  7%|â–‹         | 26/378 [00:11<02:26,  2.41it/s]\r  7%|â–‹         | 27/378 [00:11<02:25,  2.40it/s]\r  7%|â–‹         | 28/378 [00:11<02:25,  2.41it/s]\r  8%|â–Š         | 29/378 [00:12<02:24,  2.41it/s]\r  8%|â–Š         | 30/378 [00:12<02:24,  2.41it/s]\r  8%|â–Š         | 31/378 [00:13<02:24,  2.41it/s]\r  8%|â–Š         | 32/378 [00:13<02:23,  2.41it/s]\r  9%|â–Š         | 33/378 [00:13<02:23,  2.40it/s]\r  9%|â–‰         | 34/378 [00:14<02:22,  2.41it/s]\r  9%|â–‰         | 35/378 [00:14<02:22,  2.41it/s]\r 10%|â–‰         | 36/378 [00:15<02:22,  2.41it/s]\r 10%|â–‰         | 37/378 [00:15<02:21,  2.40it/s]\r 10%|â–ˆ         | 38/378 [00:15<02:21,  2.40it/s]\r 10%|â–ˆ         | 39/378 [00:16<02:20,  2.41it/s]\r 11%|â–ˆ         | 40/378 [00:16<02:20,  2.40it/s]\r 11%|â–ˆ         | 41/378 [00:17<02:20,  2.40it/s]\r 11%|â–ˆ         | 42/378 [00:17<02:19,  2.41it/s]\r 11%|â–ˆâ–        | 43/378 [00:18<02:19,  2.40it/s]\r 12%|â–ˆâ–        | 44/378 [00:18<02:18,  2.41it/s]\r 12%|â–ˆâ–        | 45/378 [00:18<02:18,  2.40it/s]\r 12%|â–ˆâ–        | 46/378 [00:19<02:17,  2.41it/s]\r 12%|â–ˆâ–        | 47/378 [00:19<02:17,  2.40it/s]\r 13%|â–ˆâ–Ž        | 48/378 [00:20<02:17,  2.41it/s]\r 13%|â–ˆâ–Ž        | 49/378 [00:20<02:16,  2.40it/s]\r 13%|â–ˆâ–Ž        | 50/378 [00:20<02:16,  2.39it/s]\r 13%|â–ˆâ–Ž        | 51/378 [00:21<02:16,  2.39it/s]\r 14%|â–ˆâ–        | 52/378 [00:21<02:16,  2.39it/s]\r 14%|â–ˆâ–        | 53/378 [00:22<02:16,  2.38it/s]\r 14%|â–ˆâ–        | 54/378 [00:22<02:15,  2.39it/s]\r 15%|â–ˆâ–        | 55/378 [00:23<02:15,  2.39it/s]\r 15%|â–ˆâ–        | 56/378 [00:23<02:15,  2.38it/s]\r 15%|â–ˆâ–Œ        | 57/378 [00:23<02:14,  2.39it/s]\r 15%|â–ˆâ–Œ        | 58/378 [00:24<02:13,  2.39it/s]\r 16%|â–ˆâ–Œ        | 59/378 [00:24<02:13,  2.38it/s]\r 16%|â–ˆâ–Œ        | 60/378 [00:25<02:13,  2.38it/s]\r 16%|â–ˆâ–Œ        | 61/378 [00:25<02:12,  2.39it/s]\r 16%|â–ˆâ–‹        | 62/378 [00:26<02:12,  2.39it/s]\r 17%|â–ˆâ–‹        | 63/378 [00:26<02:12,  2.38it/s]\r 17%|â–ˆâ–‹        | 64/378 [00:26<02:11,  2.39it/s]\r 17%|â–ˆâ–‹        | 65/378 [00:27<02:11,  2.39it/s]\r 17%|â–ˆâ–‹        | 66/378 [00:27<02:10,  2.38it/s]\r 18%|â–ˆâ–Š        | 67/378 [00:28<02:10,  2.39it/s]\r 18%|â–ˆâ–Š        | 68/378 [00:28<02:10,  2.38it/s]\r 18%|â–ˆâ–Š        | 69/378 [00:28<02:10,  2.38it/s]\r 19%|â–ˆâ–Š        | 70/378 [00:29<02:09,  2.37it/s]\r 19%|â–ˆâ–‰        | 71/378 [00:29<02:09,  2.37it/s]\r 19%|â–ˆâ–‰        | 72/378 [00:30<02:09,  2.37it/s]\r 19%|â–ˆâ–‰        | 73/378 [00:30<02:08,  2.37it/s]\r 20%|â–ˆâ–‰        | 74/378 [00:31<02:07,  2.38it/s]\r 20%|â–ˆâ–‰        | 75/378 [00:31<02:07,  2.37it/s]\r 20%|â–ˆâ–ˆ        | 76/378 [00:31<02:07,  2.37it/s]\r 20%|â–ˆâ–ˆ        | 77/378 [00:32<02:07,  2.37it/s]\r 21%|â–ˆâ–ˆ        | 78/378 [00:32<02:06,  2.37it/s]\r 21%|â–ˆâ–ˆ        | 79/378 [00:33<02:05,  2.37it/s]\r 21%|â–ˆâ–ˆ        | 80/378 [00:33<02:05,  2.37it/s]\r 21%|â–ˆâ–ˆâ–       | 81/378 [00:34<02:05,  2.37it/s]\r 22%|â–ˆâ–ˆâ–       | 82/378 [00:34<02:05,  2.36it/s]\r 22%|â–ˆâ–ˆâ–       | 83/378 [00:34<02:05,  2.36it/s]\r 22%|â–ˆâ–ˆâ–       | 84/378 [00:35<02:04,  2.36it/s]\r 22%|â–ˆâ–ˆâ–       | 85/378 [00:35<02:04,  2.35it/s]\r 23%|â–ˆâ–ˆâ–Ž       | 86/378 [00:36<02:03,  2.36it/s]\r 23%|â–ˆâ–ˆâ–Ž       | 87/378 [00:36<02:03,  2.36it/s]\r 23%|â–ˆâ–ˆâ–Ž       | 88/378 [00:37<02:03,  2.35it/s]\r 24%|â–ˆâ–ˆâ–Ž       | 89/378 [00:37<02:02,  2.35it/s]\r 24%|â–ˆâ–ˆâ–       | 90/378 [00:37<02:02,  2.35it/s]\r 24%|â–ˆâ–ˆâ–       | 91/378 [00:38<02:02,  2.34it/s]\r 24%|â–ˆâ–ˆâ–       | 92/378 [00:38<02:01,  2.35it/s]\r 25%|â–ˆâ–ˆâ–       | 93/378 [00:39<02:01,  2.35it/s]\r 25%|â–ˆâ–ˆâ–       | 94/378 [00:39<02:01,  2.34it/s]\r 25%|â–ˆâ–ˆâ–Œ       | 95/378 [00:39<02:00,  2.35it/s]\r 25%|â–ˆâ–ˆâ–Œ       | 96/378 [00:40<02:00,  2.35it/s]\r 26%|â–ˆâ–ˆâ–Œ       | 97/378 [00:40<01:59,  2.35it/s]\r 26%|â–ˆâ–ˆâ–Œ       | 98/378 [00:41<01:59,  2.35it/s]\r 26%|â–ˆâ–ˆâ–Œ       | 99/378 [00:41<01:58,  2.35it/s]\r 26%|â–ˆâ–ˆâ–‹       | 100/378 [00:42<01:58,  2.35it/s]\r 27%|â–ˆâ–ˆâ–‹       | 101/378 [00:42<01:58,  2.34it/s]\r 27%|â–ˆâ–ˆâ–‹       | 102/378 [00:42<01:57,  2.35it/s]\r 27%|â–ˆâ–ˆâ–‹       | 103/378 [00:43<01:57,  2.35it/s]\r 28%|â–ˆâ–ˆâ–Š       | 104/378 [00:43<01:56,  2.35it/s]\r 28%|â–ˆâ–ˆâ–Š       | 105/378 [00:44<01:56,  2.35it/s]\r 28%|â–ˆâ–ˆâ–Š       | 106/378 [00:44<01:56,  2.34it/s]\r 28%|â–ˆâ–ˆâ–Š       | 107/378 [00:45<01:55,  2.34it/s]\r 29%|â–ˆâ–ˆâ–Š       | 108/378 [00:45<01:55,  2.34it/s]\r 29%|â–ˆâ–ˆâ–‰       | 109/378 [00:45<01:54,  2.34it/s]\r 29%|â–ˆâ–ˆâ–‰       | 110/378 [00:46<01:54,  2.35it/s]\r 29%|â–ˆâ–ˆâ–‰       | 111/378 [00:46<01:53,  2.34it/s]\r 30%|â–ˆâ–ˆâ–‰       | 112/378 [00:47<01:53,  2.34it/s]\r 30%|â–ˆâ–ˆâ–‰       | 113/378 [00:47<01:53,  2.33it/s]\r 30%|â–ˆâ–ˆâ–ˆ       | 114/378 [00:48<01:53,  2.33it/s]\r 30%|â–ˆâ–ˆâ–ˆ       | 115/378 [00:48<01:52,  2.33it/s]\r 31%|â–ˆâ–ˆâ–ˆ       | 116/378 [00:48<01:52,  2.33it/s]\r 31%|â–ˆâ–ˆâ–ˆ       | 117/378 [00:49<01:51,  2.33it/s]\r 31%|â–ˆâ–ˆâ–ˆ       | 118/378 [00:49<01:51,  2.34it/s]\r 31%|â–ˆâ–ˆâ–ˆâ–      | 119/378 [00:50<01:50,  2.34it/s]\r 32%|â–ˆâ–ˆâ–ˆâ–      | 120/378 [00:50<01:50,  2.34it/s]\r 32%|â–ˆâ–ˆâ–ˆâ–      | 121/378 [00:51<01:49,  2.34it/s]\r 32%|â–ˆâ–ˆâ–ˆâ–      | 122/378 [00:51<01:49,  2.34it/s]\r 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 123/378 [00:51<01:49,  2.33it/s]\r 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 124/378 [00:52<01:48,  2.33it/s]\r 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 125/378 [00:52<01:48,  2.33it/s]\r 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 126/378 [00:53<01:47,  2.34it/s]\r 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 127/378 [00:53<01:47,  2.34it/s]\r 34%|â–ˆâ–ˆâ–ˆâ–      | 128/378 [00:54<01:47,  2.33it/s]\r 34%|â–ˆâ–ˆâ–ˆâ–      | 129/378 [00:54<01:46,  2.33it/s]\r 34%|â–ˆâ–ˆâ–ˆâ–      | 130/378 [00:54<01:46,  2.32it/s]\r 35%|â–ˆâ–ˆâ–ˆâ–      | 131/378 [00:55<01:46,  2.33it/s]\r 35%|â–ˆâ–ˆâ–ˆâ–      | 132/378 [00:55<01:45,  2.33it/s]\r 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/378 [00:56<01:45,  2.33it/s]\r 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 134/378 [00:56<01:45,  2.32it/s]\r 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 135/378 [00:57<01:44,  2.32it/s]\r 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 136/378 [00:57<01:43,  2.33it/s]\r 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 137/378 [00:57<01:43,  2.32it/s]\r 37%|â–ˆâ–ˆâ–ˆâ–‹      | 138/378 [00:58<01:43,  2.32it/s]\r 37%|â–ˆâ–ˆâ–ˆâ–‹      | 139/378 [00:58<01:43,  2.32it/s]\r 37%|â–ˆâ–ˆâ–ˆâ–‹      | 140/378 [00:59<01:43,  2.31it/s]\r 37%|â–ˆâ–ˆâ–ˆâ–‹      | 141/378 [00:59<01:42,  2.31it/s]\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 142/378 [01:00<01:42,  2.31it/s]\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 143/378 [01:00<01:41,  2.31it/s]\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 144/378 [01:00<01:41,  2.31it/s]\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 145/378 [01:01<01:41,  2.30it/s]\r 39%|â–ˆâ–ˆâ–ˆâ–Š      | 146/378 [01:01<01:40,  2.31it/s]\r 39%|â–ˆâ–ˆâ–ˆâ–‰      | 147/378 [01:02<01:40,  2.30it/s]\r 39%|â–ˆâ–ˆâ–ˆâ–‰      | 148/378 [01:02<01:39,  2.30it/s]\r 39%|â–ˆâ–ˆâ–ˆâ–‰      | 149/378 [01:03<01:39,  2.31it/s]\r 40%|â–ˆâ–ˆâ–ˆâ–‰      | 150/378 [01:03<01:39,  2.30it/s]\r 40%|â–ˆâ–ˆâ–ˆâ–‰      | 151/378 [01:04<01:38,  2.31it/s]\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 152/378 [01:04<01:38,  2.30it/s]\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 153/378 [01:04<01:38,  2.29it/s]\r 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 154/378 [01:05<01:37,  2.29it/s]\r 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 155/378 [01:05<01:37,  2.28it/s]\r 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/378 [01:06<01:37,  2.29it/s]\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/378 [01:06<01:36,  2.29it/s]\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/378 [01:07<01:36,  2.28it/s]\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/378 [01:07<01:35,  2.28it/s]\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/378 [01:07<01:35,  2.29it/s]\r 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 161/378 [01:08<01:34,  2.29it/s]\r 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 162/378 [01:08<01:34,  2.29it/s]\r 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 163/378 [01:09<01:34,  2.28it/s]\r 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 164/378 [01:09<01:33,  2.29it/s]\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 165/378 [01:10<01:33,  2.28it/s]\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 166/378 [01:10<01:32,  2.29it/s]\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 167/378 [01:11<01:32,  2.29it/s]\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 168/378 [01:11<01:31,  2.28it/s]\r 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 169/378 [01:11<01:31,  2.28it/s]\r 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 170/378 [01:12<01:31,  2.28it/s]\r 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 171/378 [01:12<01:30,  2.28it/s]\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 172/378 [01:13<01:30,  2.29it/s]\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 173/378 [01:13<01:29,  2.28it/s]\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 174/378 [01:14<01:29,  2.28it/s]\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 175/378 [01:14<01:28,  2.28it/s]\r 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 176/378 [01:14<01:28,  2.27it/s]\r 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 177/378 [01:15<01:28,  2.27it/s]\r 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 178/378 [01:15<01:28,  2.27it/s]\r 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 179/378 [01:16<01:27,  2.27it/s]\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 180/378 [01:16<01:27,  2.27it/s]\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 181/378 [01:17<01:26,  2.27it/s]\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 182/378 [01:17<01:26,  2.27it/s]\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 183/378 [01:18<01:26,  2.27it/s]\r 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 184/378 [01:18<01:25,  2.27it/s]\r 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 185/378 [01:18<01:25,  2.26it/s]\r 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 186/378 [01:19<01:24,  2.26it/s]\r 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 187/378 [01:19<01:24,  2.26it/s]\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 188/378 [01:20<01:24,  2.26it/s]\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 189/378 [01:20<01:23,  2.25it/s]\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 190/378 [01:21<01:23,  2.25it/s]\r 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 191/378 [01:21<01:22,  2.25it/s]\r 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 192/378 [01:22<01:22,  2.25it/s]\r 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 193/378 [01:22<01:22,  2.25it/s]\r 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/378 [01:22<01:21,  2.25it/s]\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/378 [01:23<01:21,  2.25it/s]\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/378 [01:23<01:20,  2.25it/s]\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/378 [01:24<01:20,  2.25it/s]\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/378 [01:24<01:20,  2.25it/s]\r 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 199/378 [01:25<01:19,  2.25it/s]\r 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 200/378 [01:25<01:19,  2.24it/s]\r 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 201/378 [01:26<01:18,  2.25it/s]\r 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 202/378 [01:26<01:18,  2.25it/s]\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 203/378 [01:26<01:17,  2.25it/s]\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 204/378 [01:27<01:17,  2.24it/s]\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 205/378 [01:27<01:17,  2.24it/s]\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 206/378 [01:28<01:16,  2.25it/s]\r 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 207/378 [01:28<01:16,  2.25it/s]\r 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 208/378 [01:29<01:15,  2.24it/s]\r 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 209/378 [01:29<01:15,  2.24it/s]\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 210/378 [01:30<01:15,  2.24it/s]\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 211/378 [01:30<01:14,  2.23it/s]\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 212/378 [01:30<01:14,  2.23it/s]\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 213/378 [01:31<01:13,  2.23it/s]\r 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 214/378 [01:31<01:13,  2.23it/s]\r 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 215/378 [01:32<01:13,  2.23it/s]\r 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 216/378 [01:32<01:12,  2.23it/s]\r 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 217/378 [01:33<01:12,  2.22it/s]\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 218/378 [01:33<01:12,  2.22it/s]\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 219/378 [01:34<01:11,  2.22it/s]\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 220/378 [01:34<01:11,  2.22it/s]\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 221/378 [01:35<01:10,  2.22it/s]\r 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 222/378 [01:35<01:10,  2.22it/s]\r 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 223/378 [01:35<01:10,  2.21it/s]\r 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 224/378 [01:36<01:09,  2.21it/s]\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 225/378 [01:36<01:09,  2.21it/s]\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 226/378 [01:37<01:08,  2.21it/s]\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 227/378 [01:37<01:08,  2.21it/s]\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 228/378 [01:38<01:07,  2.21it/s]\r 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 229/378 [01:38<01:07,  2.20it/s]\r 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 230/378 [01:39<01:07,  2.20it/s]\r 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 231/378 [01:39<01:06,  2.20it/s]\r 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/378 [01:40<01:06,  2.20it/s]\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/378 [01:40<01:05,  2.20it/s]\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/378 [01:40<01:05,  2.20it/s]\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/378 [01:41<01:05,  2.20it/s]\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/378 [01:41<01:04,  2.20it/s]\r 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 237/378 [01:42<01:04,  2.19it/s]\r 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 238/378 [01:42<01:04,  2.18it/s]\r 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 239/378 [01:43<01:03,  2.18it/s]\r 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 240/378 [01:43<01:03,  2.18it/s]\r 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 241/378 [01:44<01:02,  2.19it/s]\r 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 242/378 [01:44<01:02,  2.18it/s]\r 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 243/378 [01:45<01:02,  2.18it/s]\r 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 244/378 [01:45<01:01,  2.17it/s]\r 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 245/378 [01:46<01:01,  2.17it/s]\r 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 246/378 [01:46<01:00,  2.17it/s]\r 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 247/378 [01:46<01:00,  2.17it/s]\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 248/378 [01:47<00:59,  2.17it/s]\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 249/378 [01:47<00:59,  2.17it/s]\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 250/378 [01:48<00:59,  2.17it/s]\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 251/378 [01:48<00:58,  2.17it/s]\r 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 252/378 [01:49<00:58,  2.17it/s]\r 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 253/378 [01:49<00:57,  2.17it/s]\r 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 254/378 [01:50<00:57,  2.16it/s]\r 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 255/378 [01:50<00:56,  2.16it/s]\r 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 256/378 [01:51<00:56,  2.17it/s]\r 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 257/378 [01:51<00:55,  2.16it/s]\r 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 258/378 [01:52<00:55,  2.16it/s]\r 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 259/378 [01:52<00:55,  2.15it/s]\r 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 260/378 [01:52<00:54,  2.15it/s]\r 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 261/378 [01:53<00:54,  2.15it/s]\r 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 262/378 [01:53<00:53,  2.15it/s]\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 263/378 [01:54<00:53,  2.15it/s]\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 264/378 [01:54<00:52,  2.15it/s]\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 265/378 [01:55<00:52,  2.16it/s]\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 266/378 [01:55<00:51,  2.16it/s]\r 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 267/378 [01:56<00:51,  2.16it/s]\r 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 268/378 [01:56<00:51,  2.15it/s]\r 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 269/378 [01:57<00:50,  2.15it/s]\r 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/378 [01:57<00:50,  2.16it/s]\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/378 [01:58<00:49,  2.15it/s]\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/378 [01:58<00:49,  2.14it/s]\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/378 [01:58<00:48,  2.14it/s]\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/378 [01:59<00:48,  2.15it/s]\r 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 275/378 [01:59<00:47,  2.15it/s]\r 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 276/378 [02:00<00:47,  2.15it/s]\r 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 277/378 [02:00<00:46,  2.16it/s]\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 278/378 [02:01<00:46,  2.16it/s]\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 279/378 [02:01<00:45,  2.17it/s]\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 280/378 [02:02<00:45,  2.16it/s]\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 281/378 [02:02<00:44,  2.16it/s]\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 282/378 [02:03<00:44,  2.17it/s]\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 283/378 [02:03<00:43,  2.17it/s]\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 284/378 [02:04<00:43,  2.17it/s]\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 285/378 [02:04<00:42,  2.17it/s]\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 286/378 [02:04<00:42,  2.18it/s]\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 287/378 [02:05<00:41,  2.18it/s]\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 288/378 [02:05<00:41,  2.18it/s]\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 289/378 [02:06<00:40,  2.18it/s]\r 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 290/378 [02:06<00:40,  2.19it/s]\r 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 291/378 [02:07<00:39,  2.18it/s]\r 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 292/378 [02:07<00:39,  2.18it/s]\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 293/378 [02:08<00:38,  2.19it/s]\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 294/378 [02:08<00:38,  2.17it/s]\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 295/378 [02:09<00:38,  2.18it/s]\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 296/378 [02:09<00:37,  2.18it/s]\r 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 297/378 [02:10<00:36,  2.19it/s]\r 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 298/378 [02:10<00:36,  2.19it/s]\r 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 299/378 [02:10<00:35,  2.20it/s]\r 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 300/378 [02:11<00:35,  2.21it/s]\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 301/378 [02:11<00:34,  2.20it/s]\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 302/378 [02:12<00:34,  2.21it/s]\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 303/378 [02:12<00:33,  2.21it/s]\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 304/378 [02:13<00:33,  2.21it/s]\r 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 305/378 [02:13<00:33,  2.21it/s]\r 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 306/378 [02:14<00:32,  2.21it/s]\r 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 307/378 [02:14<00:32,  2.21it/s]\r 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/378 [02:14<00:31,  2.21it/s]\r 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/378 [02:15<00:31,  2.21it/s]\r 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/378 [02:15<00:30,  2.21it/s]\r 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/378 [02:16<00:30,  2.21it/s]\r 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 312/378 [02:16<00:29,  2.21it/s]\r 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 313/378 [02:17<00:29,  2.21it/s]\r 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 314/378 [02:17<00:28,  2.21it/s]\r 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 315/378 [02:18<00:28,  2.22it/s]\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 316/378 [02:18<00:27,  2.22it/s]\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 317/378 [02:19<00:27,  2.22it/s]\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 318/378 [02:19<00:27,  2.22it/s]\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 319/378 [02:19<00:26,  2.22it/s]\r 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 320/378 [02:20<00:26,  2.23it/s]\r 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 321/378 [02:20<00:25,  2.23it/s]\r 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 322/378 [02:21<00:25,  2.22it/s]\r 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 323/378 [02:21<00:24,  2.22it/s]\r 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 324/378 [02:22<00:24,  2.22it/s]\r 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 325/378 [02:22<00:23,  2.23it/s]\r 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 326/378 [02:23<00:23,  2.23it/s]\r 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 327/378 [02:23<00:22,  2.22it/s]\r 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 328/378 [02:23<00:22,  2.22it/s]\r 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 329/378 [02:24<00:22,  2.22it/s]\r 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 330/378 [02:24<00:21,  2.22it/s]\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 331/378 [02:25<00:21,  2.23it/s]\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 332/378 [02:25<00:20,  2.23it/s]\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 333/378 [02:26<00:20,  2.22it/s]\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 334/378 [02:26<00:19,  2.22it/s]\r 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 335/378 [02:27<00:19,  2.22it/s]\r 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 336/378 [02:27<00:18,  2.23it/s]\r 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 337/378 [02:28<00:18,  2.23it/s]\r 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 338/378 [02:28<00:17,  2.24it/s]\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 339/378 [02:28<00:17,  2.24it/s]\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 340/378 [02:29<00:16,  2.24it/s]\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 341/378 [02:29<00:16,  2.24it/s]\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 342/378 [02:30<00:16,  2.23it/s]\r 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 343/378 [02:30<00:15,  2.23it/s]\r 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 344/378 [02:31<00:15,  2.23it/s]\r 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/378 [02:31<00:14,  2.23it/s]\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/378 [02:32<00:14,  2.23it/s]\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/378 [02:32<00:13,  2.23it/s]\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/378 [02:32<00:13,  2.24it/s]\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 349/378 [02:33<00:12,  2.24it/s]\r 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 350/378 [02:33<00:12,  2.24it/s]\r 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 351/378 [02:34<00:12,  2.23it/s]\r 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 352/378 [02:34<00:11,  2.23it/s]\r 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 353/378 [02:35<00:11,  2.23it/s]\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 354/378 [02:35<00:10,  2.23it/s]\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 355/378 [02:36<00:10,  2.23it/s]\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 356/378 [02:36<00:09,  2.23it/s]\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 357/378 [02:36<00:09,  2.23it/s]\r 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 358/378 [02:37<00:08,  2.23it/s]\r 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 359/378 [02:37<00:08,  2.23it/s]\r 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 360/378 [02:38<00:08,  2.22it/s]\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 361/378 [02:38<00:07,  2.22it/s]\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 362/378 [02:39<00:07,  2.23it/s]\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 363/378 [02:39<00:06,  2.23it/s]\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 364/378 [02:40<00:06,  2.23it/s]\r 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 365/378 [02:40<00:05,  2.23it/s]\r 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 366/378 [02:41<00:05,  2.22it/s]\r 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 367/378 [02:41<00:04,  2.22it/s]\r 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 368/378 [02:41<00:04,  2.22it/s]\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 369/378 [02:42<00:04,  2.23it/s]\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 370/378 [02:42<00:03,  2.23it/s]\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 371/378 [02:43<00:03,  2.23it/s]\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 372/378 [02:43<00:02,  2.23it/s]\r 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 373/378 [02:44<00:02,  2.22it/s]\r 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 374/378 [02:44<00:01,  2.22it/s]\r 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 375/378 [02:45<00:01,  2.22it/s]\r 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 376/378 [02:45<00:00,  2.22it/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 377/378 [02:45<00:00,  2.21it/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 378/378 [02:46<00:00,  2.22it/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 378/378 [02:46<00:00,  2.27it/s]\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","10/24/2020 20:18:46 - INFO - __main__ -   *** Evaluate ***\n","\r  0%|          | 0/20 [00:00<?, ?it/s]\r 10%|â–ˆ         | 2/20 [00:01<00:09,  1.81it/s]\r 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.39it/s]\r 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.19it/s]\r 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:13,  1.08it/s]\r 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:13,  1.01it/s]\r 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:13,  1.03s/it]\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:12,  1.06s/it]\r 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:11,  1.08s/it]\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:10<00:10,  1.10s/it]\r 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:11<00:09,  1.11s/it]\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:12<00:08,  1.12s/it]\r 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:13<00:07,  1.12s/it]\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:14<00:06,  1.13s/it]\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:15<00:05,  1.13s/it]\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:16<00:04,  1.13s/it]\r 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:18<00:03,  1.14s/it]\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:19<00:02,  1.14s/it]\r 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:20<00:01,  1.14s/it]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.08s/it]10/24/2020 20:19:08 - INFO - __main__ -   ***** Eval results *****\n","10/24/2020 20:19:08 - INFO - __main__ -     perplexity = 1.7268832142682369\n","\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.08s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xd-on4Cpw4uQ","executionInfo":{"status":"error","timestamp":1603681754923,"user_tz":300,"elapsed":9484,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"49786494-fd3e-46d4-ef26-af3f9b131a75","colab":{"base_uri":"https://localhost:8080/","height":463}},"source":["### PYTHON EQUIVALENT OF ABOVE ###\n","\n","from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n","from transformers import glue_convert_examples_to_features\n","from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n","from datasets import load_dataset\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","model_name = \"sshleifer/distilbart-cnn-12-6\"\n","model = AutoModelWithLMHead.from_pretrained(model_name)\n","# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","newsroom = load_dataset('newsroom', data_dir='/content/drive/My Drive/CS196Project/newsroom')\n","\n","train_dataset = glue_convert_examples_to_features(newsroom['train'], tokenizer, max_length=128, task='mrpc')\n","train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n","\n","validation_dataset = glue_convert_examples_to_features(newsroom['validation'], tokenizer, max_length=128, task='mrpc')\n","validation_dataset = validation_dataset.shuffle(100).batch(32).repeat(2)\n","\n","training_args = TrainingArguments(\n","    output_dir='/content/drive/My Drive/CS196Project/results',          # output directory\n","    num_train_epochs=2,              # total # of training epochs\n","    per_device_train_batch_size=1,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='/content/drive/My Drive/CS196Project/logs',            # directory for storing logs\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=validation_dataset      # evaluation dataset\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:825: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","Using custom data configuration default\n","Reusing dataset newsroom (/root/.cache/huggingface/datasets/newsroom/default/1.0.0/4b405ccd64e15f685065870ea563a1e6a034d1bd269a5427f40146d81549095e)\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-7aff3e985474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnewsroom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newsroom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/CS196Project/newsroom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mrpc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36mglue_convert_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, task, label_list, output_mode)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_glue_convert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     return _glue_convert_examples_to_features(\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36m_glue_convert_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, task, label_list, output_mode)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     batch_encoding = tokenizer(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     batch_encoding = tokenizer(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36mlabel_from_example\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlabel_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'label'"]}]},{"cell_type":"markdown","metadata":{"id":"s0Fwb6TEUgj1"},"source":["## Results\n","\n","To use it, you can run something like"]},{"cell_type":"code","metadata":{"id":"EGAlyaB3Sfiu","executionInfo":{"status":"ok","timestamp":1603571204030,"user_tz":300,"elapsed":9514,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"a414789f-c254-4284-b3d8-162a599bf2b1","colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n","import torch\n","import numpy as np\n","\n","OUTPUT_DIR = \"./output_mine\"\n","device = 'cpu'\n","\n","# tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n","# model = AutoModelWithLMHead.from_pretrained(OUTPUT_DIR)\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"yuvraj/summarizer-cnndm\") \n","model = AutoModelWithLMHead.from_pretrained(\"yuvraj/summarizer-cnndm\")\n","\n","# model = model.to(device)\n","model = model.to(device)\n","\n","# summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n","summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n","                                        \n","# def generate(input_str, model=model, tokenizer=tokenizer, length=250, n=5):\n","#   output_text = input_str\n","#   summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n","#   for word_num in range(length):\n","#     data = unmasker(output_text)\n","#     probs = []\n","#     current_prob = 0\n","#     for word in data:\n","#       current_prob += word['score']\n","#       probs.append((current_prob, word['token_str']))\n","#     choice = np.random.uniform(high=current_prob)\n","#     probs.append(choice, \"\")\n","#     probs = sorted(probs)\n","#     index = probs.index((choice, \"\"))\n","#     output_text = output_text + probs[index-1][1] + \" \"\n","#   return output_text\n","\n","# generated_text = generate(\" = University of Illinois = \\n\")\n","# print(generated_text)\n","\n","text = newsroom[0]['text'].split(' ')\n","text = ' '.join(text[:691])\n","\n","print(text + \"\\n\")\n","print(summarizer(text))\n","\n","print(\"\\n\\n\")\n","\n","# summarizer(text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:825: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["HAMBURG, Germany, June 3 Â— As he left the soccer field after a club match in the eastern German city of Halle on March 25, the Nigerian forward Adebowale Ogungbure was spit upon, jeered with racial remarks and mocked with monkey noises. In rebuke, he placed two fingers under his nose to simulate a Hitler mustache and thrust his arm in a Nazi salute.\n","\n","Marc Zoro, right, an Ivory Coast native, was a target of racial slurs from the home fans in Messina, Italy. Adriano, a star with Inter Milan, tried to persuade him to stay on the field.\n","\n","From now until its conclusion on July 9, Jeff Z. Klein and other staff members of The Times and International Herald Tribune will track the world's most popular sporting event.\n","\n","Your guide to the games in Germany: teams, rosters, schedules, statistics, venues and more.\n","\n","In April, the American defender Oguchi Onyewu, playing for his professional club team in Belgium, dismissively gestured toward fans who were making simian chants at him. Then, as he went to throw the ball inbounds, Onyewu said a fan of the opposing team reached over a barrier and punched him in the face.\n","\n","International soccer has been plagued for years by violence among fans, including racial incidents. But FIFA, soccer's Zurich-based world governing body, said there has been a recent surge in discriminatory behavior toward blacks by fans and other players, an escalation that has dovetailed with the signing of more players from Africa and Latin America by elite European clubs.\n","\n","This \"deplorable trend,\" as FIFA has called it, now threatens to embarrass the sport on its grandest stage, the World Cup, which opens June 9 for a monthlong run in 12 cities around Germany. More than 30 billion cumulative television viewers are expected to watch part of the competition and Joseph S. Blatter, FIFA's president, has vowed to crack down on racist behavior during the tournament.\n","\n","Underlining FIFA's concerns, the issue has been included on the agenda at its biannual Congress, scheduled to be held this week in Munich. A campaign against bigotry includes \"Say No to Racism\" stadium banners, television commercials, and team captains making pregame speeches during the quarterfinals of the 32-team tournament.\n","\n","Players, coaches and officials have been threatened with sanctions. But FIFA has said it would not be practical to use the harshest penalties available to punish misbehaving fans Â— halting matches, holding games in empty stadiums and deducting points that teams receive for victories and ties.\n","\n","Players and antiracism experts said they expected offensive behavior during the tournament, including monkey-like chanting; derisive singing; the hanging of banners that reflect neofascist and racist beliefs; and perhaps the tossing of bananas or banana peels, all familiar occurrences during matches in Spain, Italy, eastern Germany and eastern Europe.\n","\n","\"For us it's quite clear this is a reflection of underlying tensions that exist in European societies,\" said Piara Powar, director of the London-based antiracist soccer organization Kick It Out. He said of Eastern Europe: \"Poverty, unemployment, is a problem. Indigenous people are looking for easy answers to blame. Often newcomers bear the brunt of the blame.\"\n","\n","Yet experts and players also said they believed the racist behavior would be more constrained at the World Cup than it was during play in various domestic leagues around Europe, because of increased security, the international makeup of the crowds, higher ticket prices and a sense that spectators would be generally well behaved on soccer's grandest stage.\n","\n","\"We have to differentiate inside and outside the stadium,\" said Kurt Wachter, project coordinator for the Vienna-based Football Against Racism in Europe, a network of organizations that seeks to fight bigotry and xenophobia in 35 countries.\n","\n","\"Racism is a feature of many football leagues inside and outside Europe,\" said Wachter, who expects most problems to occur outside stadiums where crowds are less controlled. \"We're sure we will see some things we're used to seeing. It won't stop because of the World Cup.\"\n","\n","Particularly worrisome are the possibilities of attacks by extremist groups on spectators and visitors in train stations, bars, restaurants and open areas near the stadiums, Wachter and other experts said. To promote tolerance, he said his organization would organize street soccer matches outside World Cup\n","\n","[{'summary_text': 'FIFA says it will not use the harshest penalties available to punish misbehaving fans .'}]\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KvLfcwTOUnlt"},"source":["## Compressing/Zipping Model\n","\n","\n","In order for us to preserve this model, we should compress it and save it somewhere. This can be done easily with"]},{"cell_type":"code","metadata":{"id":"W4zDMZZdUkW2"},"source":["! tar -czf gpt2-tuned.tar.gz output/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LT8ffxEsVL0-"},"source":["which creates a file called `gpt2-tuned.tar.gz`"]},{"cell_type":"markdown","metadata":{"id":"8vGGDE3bVO6s"},"source":["## Saving it to GoogleÂ Drive"]},{"cell_type":"code","metadata":{"id":"X9J6NUCkVPpn","executionInfo":{"status":"ok","timestamp":1591995754331,"user_tz":240,"elapsed":1229,"user":{"displayName":"Joey Sham","photoUrl":"","userId":"07509763358611462220"}},"outputId":"92ecdae4-67e4-4d90-e91d-2cea4e5f1411","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_MmLCVxSXDc-"},"source":["Now you can copy your output model to your Google Drive by running"]},{"cell_type":"code","metadata":{"id":"XyUwAAqJXMxS"},"source":["!cp gpt2-tuned.tar.gz /content/drive/My\\ Drive/"],"execution_count":null,"outputs":[]}]}