{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"dataset_merge.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ej9bxb_ytgqs"},"source":["# Install any package dependencies\n","# !pip install datasets\n","\n","# Import any required libraries\n","from datasets import load_dataset\n","from google.colab import drive\n","import os\n","\n","# IF NEWS ROOM IS NOT CACHED YET, use the following link to request the full dataset: https://cornell.qualtrics.com/jfe/form/SV_6YA3HQ2p75XH4IR\n","# You will receive an email to download the dataset. Download, unzip, and uncompress the files from google drive \n","# replace the filepath below if needed to store the dataset in the huggingface cache \n","\n","# # Use google drive to store initial dataset before caching (if required by the dataset):\n","# drive.mount('/content/drive')\n","# # Load dataset and store in cache:\n","# ds = load_dataset('newsroom', split='test', data_dir='/content/drive/My Drive/newsroom')\n","\n","# load all other datasets here...\n","newsroom = load_dataset('newsroom', split='train')\n","arxiv = load_dataset('scientific_papers', 'arxiv', split='train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNqQp1lKtgqw"},"source":["# Use this dictionary to store the keys used to acces summary/text in each dataset\n","# This is to account for different naming conventions between the datasets we use\n","\n","dataset_keys = {\n","    'newsroom': {\n","        'DATASET_OBJ': ds,\n","        'TEXT': 'text',\n","        'SMRY': 'summary'\n","    },\n","    'arXiv': {\n","        # ... fill in the rest\n","    } # ...add all other datasets\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B9kY3lNtgq1"},"source":["def make_dataset_string(split='train', demo_mode=True):\n","  assert split in ['train', 'test']\n","  # Change <demo_mode> to False if you want to process each ENTIRE dataset \n","\n","  if demo_mode:\n","      print('DEMO MODE ACTIVE\\n\\n')\n","\n","  # Currently, this code will store the processed datasets as a list of entries \n","  # Each entry is stored as a processed string in the form of:\n","  # [TEXT]\n","  # blah blah\n","  # ...\n","  # blah blah\n","  # [SMRY]\n","  # blah blah\n","  # \n","  # <START OVER AGAIN>  --> This line is not in the string \n","  #                     --> indicates how formatting would look if the entries were concatenated\n","  #                     --> entries DO CONTAIN THEIR OWN NEWLINES\n","\n","  processed_datasets = { }\n","\n","  for name in dataset_dict_keys.keys():\n","      # access respective keys to account for naming conventions\n","      ds_keys = dataset_keys[name]\n","      ds_obj = ds_keys['DATASET_OBJ']\n","      \n","      processed_entries = []\n","      for index, entry in enumerate(ds_obj):\n","          # Get text and prepare for processing\n","          text = entry[ds_keys['TEXT']]\n","          smry = entry[ds_keys['SMRY']]\n","          assert type(text) == str and type(smry) == str\n","          # concatenate break indicators and actual text/summaries\n","          processed_entries.append(''.join(['[TEXT]\\n', text, '\\n[SMRY]\\n', smry, '\\n\\n']))\n","          if not index % 1000:\n","              print('Processing Dataset \\\"%s\\\" at Index %d' % (name, index))\n","              if demo_mode and index == 2000:\n","                  break\n","      print('\\nFINISHED Processing Dataset \\\"%s\\\"\\n' % name)\n","      processed_datasets[name] = processed_entries\n","\n","  return processed_datasets\n","\n","# To combine one dataset into a single string for training/testing, use the following:\n","# ''.join(processed_datasets[<name_of_dataset>])\n","# To combine the entire dictionary of datasets into a single string, use the following:\n","# ''.join([''.join(processed_datasets[name]) for name in processed_datasets.keys()])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMZLiy0Xtgq3","executionInfo":{"status":"ok","timestamp":1602447434918,"user_tz":300,"elapsed":533,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"88d3e7e9-3236-45ca-e1dc-5ce1cca5a5b1","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["from google.colab import drive\n","from getpass import getpass\n","import os\n","%cd '/content/drive/My Drive/CS196Project/Group6'\n","\n","uname = \"nkaush\"\n","!git config --global user.email '$neil.kaushikkar@gmail.com'\n","!git config --global user.name '$uname'\n","# password = getpass('Password:')\n","\n","!git commit -am"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/CS196Project/Group6\n","On branch neilk3\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   dataset_merge.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"],"name":"stdout"}]}]}