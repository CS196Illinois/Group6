{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"dataset_merge.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ej9bxb_ytgqs"},"source":["# Install any package dependencies\n","# !pip install datasets\n","\n","# Import any required libraries\n","from datasets import load_dataset\n","from google.colab import drive\n","import os\n","\n","# IF NEWS ROOM IS NOT CACHED YET, use the following link to request the full dataset: https://cornell.qualtrics.com/jfe/form/SV_6YA3HQ2p75XH4IR\n","# You will receive an email to download the dataset. Download, unzip, and uncompress the files from google drive \n","# replace the filepath below if needed to store the dataset in the huggingface cache \n","\n","# # Use google drive to store initial dataset before caching (if required by the dataset):\n","drive.mount('/content/drive')\n","\n","# load all other datasets here...\n","newsroom = load_dataset('newsroom', split='train', data_dir='/content/drive/My Drive/newsroom')\n","arxiv = load_dataset('scientific_papers', 'arxiv', split='test[:1]')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNqQp1lKtgqw"},"source":["# Use this dictionary to store the keys used to acces summary/text in each dataset\n","# This is to account for different naming conventions between the datasets we use\n","\n","dataset_keys = {\n","    'newsroom': {\n","        'DATASET_OBJ': ds,\n","        'TEXT': 'text',\n","        'SMRY': 'summary'\n","    },\n","    'arXiv': {\n","        # ... fill in the rest\n","    } # ...add all other datasets\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B9kY3lNtgq1"},"source":["def make_dataset_string(split='train', demo_mode=True):\n","  assert split in ['train', 'test']\n","  # Change <demo_mode> to False if you want to process each ENTIRE dataset \n","\n","  if demo_mode:\n","      print('DEMO MODE ACTIVE\\n\\n')\n","\n","  # Currently, this code will store the processed datasets as a list of entries \n","  # Each entry is stored as a processed string in the form of:\n","  # [TEXT]\n","  # blah blah\n","  # ...\n","  # blah blah\n","  # [SMRY]\n","  # blah blah\n","  # \n","  # <START OVER AGAIN>  --> This line is not in the string \n","  #                     --> indicates how formatting would look if the entries were concatenated\n","  #                     --> entries DO CONTAIN THEIR OWN NEWLINES\n","\n","  processed_datasets = { }\n","\n","  for name in dataset_keys.keys():\n","      # access respective keys to account for naming conventions\n","      ds_keys = dataset_keys[name]\n","      ds_obj = ds_keys['DATASET_OBJ']\n","      \n","      f = open('%s.txt' % name, 'w+')\n","      for index, entry in enumerate(ds_obj):\n","          # Get text and prepare for processing\n","          text = entry[ds_keys['TEXT']]\n","          text = ''.join(text.split(\"\\n\"))\n","          smry = entry[ds_keys['SMRY']]\n","          assert type(text) == str and type(smry) == str\n","          # concatenate break indicators and actual text/summaries\n","          f.write(''.join(['[TEXT]\\n', text, '\\n[SUMM]\\n', smry, '[SEP]\\n\\n']))\n","          # if index > 200:\n","          #   break\n","          if not index % 100000:\n","              print('Processing Dataset \\\"%s\\\" at Index %d' % (name, index))\n","              if demo_mode and index == 2000:\n","                  break\n","      print('\\nFINISHED Processing Dataset \\\"%s\\\"\\n' % name)\n","      f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyzkVw0IVN1r","executionInfo":{"status":"ok","timestamp":1602716424506,"user_tz":300,"elapsed":3963,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"26ad7441-5189-4a00-9dc1-b17875498033","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# convert csv of summaries to json file\n","import pandas as pd\n","\n","%cd '/content/drive/My Drive/CS196Project/Group6'\n","\n","def to_json():\n","  df_smry = pd.read_csv('summaries.csv')\n","  smry_dict = df_smry.to_json(path_or_buf='summary_dataset.json', orient=\"records\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","/content/drive/My Drive/CS196Project/Group6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NMZLiy0Xtgq3","executionInfo":{"status":"ok","timestamp":1603571836550,"user_tz":300,"elapsed":47770,"user":{"displayName":"Neil Kaushikkar","photoUrl":"","userId":"04245653301361239178"}},"outputId":"b02493bd-3ea4-490b-e957-f86f74a64ea8","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Configure Drive/Colab/GitHub Here\n","# Use notes in text cell below if needed\n","\n","from google.colab import drive\n","from getpass import getpass\n","import os\n","\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/CS196Project/Group6'\n","\n","uname = \"nkaush\"\n","!git config --global user.email '$neil.kaushikkar@gmail.com'\n","!git config --global user.name '$uname'\n","# password = getpass('Password:')\n","\n","!git add summarization_model_output/\n","# !git commit -m \"Added summarization model and training\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/CS196Project/Group6\n","^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LuVtkzyxUL1v"},"source":["Step 1: Prepare Drive/Colab:\n","```\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/<desired path to project>'\n","```\n","Step 2: Clone GitHub Repo in Drive:\n","```\n","uname = \"<username>\"\n","!git config --global user.email '$<someone>@<email>.com'\n","!git config --global user.name '$uname'\n","password = getpass('Password:')\n","\n","!git clone https://$uname:$password@github.com/CS196Illinois/Group6\n","%cd Group6\n","```\n","Step 3: Add, Commit, Push, etc...:\n","```\n","!git add <file>\n","!git commit -m 'commit message'\n","!git push origin <branch>\n","```\n","[OPTIONAL] Step 4: If Authentication Fails:\n","```\n","!git remote -v \n","!git remote remove origin \n","!git remote add origin https://$uname:$password@github.com/CS196Illinois/Group6.git\n","```\n","Notes Compiled From:\n","- [Push from Colab to GitHub](https://stackoverflow.com/questions/59454990/how-to-push-from-colab-to-github)\n","\n","- [Git Push Authentication Failed](https://stackoverflow.com/questions/17659206/git-push-results-in-authentication-failed)\n"]}]}